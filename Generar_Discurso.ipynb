{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea Del Discurso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Integrantes**: Nicolás Cenzano, Juan Araya y Andrea Engdahl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observación** Los archivos deben estar en codificación utf-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import es_core_news_sm\n",
    "import regex\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from string import punctuation\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En caso de pedir o tener problema con los puntos, es necesario correr la siguiente línea.\n",
    "#nltk.download('punkt') #Para los puntos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usan varias de las funciones realizadas por el profesor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrearEspacioLSA(corpus,numDim,NombreModelo):\n",
    "  textos = PreProcesar(corpus)\n",
    "  transf = TfidfVectorizer()\n",
    "  tf = transf.fit_transform(textos).T\n",
    "  U, Sigma, VT = np.linalg.svd(tf.toarray())\n",
    "  terms = np.dot(U[:,:numDim], np.diag(Sigma[:numDim]))\n",
    "  docs = np.dot(np.diag(Sigma[:numDim]), VT[:numDim, :]).T \n",
    "  vocab = transf.get_feature_names()\n",
    "  GrabarModeloLSA(NombreModelo, Sigma, terms, docs, vocab)\n",
    "  #return(textos)\n",
    "\n",
    "def GrabarModeloLSA(NombreModelo,Sigma,terms,docs, vocab):\n",
    "   existe = os.path.isdir(NombreModelo)\n",
    "   if not existe:\n",
    "       os.mkdir(NombreModelo)\n",
    "   joblib.dump(Sigma,   NombreModelo +\"/\"+'sigma.pkl') \n",
    "   joblib.dump(terms,   NombreModelo +\"/\"+'terms.pkl') \n",
    "   joblib.dump(docs,    NombreModelo +\"/\"+'docs.pkl') \n",
    "   joblib.dump(vocab,   NombreModelo +\"/\"+'vocab.pkl') \n",
    "\n",
    "    \n",
    "def CargarModeloLSA(NombreModelo):\n",
    "    sigma   = joblib.load(NombreModelo+\"/\"+'sigma.pkl')\n",
    "    terms   = joblib.load(NombreModelo+\"/\"+'terms.pkl')\n",
    "    docs    = joblib.load(NombreModelo+\"/\"+'docs.pkl')\n",
    "    vocab   = joblib.load(NombreModelo+\"/\"+'vocab.pkl')\n",
    "    return(sigma, terms, docs, vocab)\n",
    "\n",
    "def CrearDiccionario(Vectores,vocabulario):\n",
    "   dicc = {}\n",
    "   for  v in range(0,len(vocabulario)):\n",
    "      dicc[vocabulario[v]] = Vectores[v]\n",
    "   return(dicc)\n",
    "\n",
    "\n",
    "def CrearCorpus(path):\n",
    "  directorio = os.listdir(path)\n",
    "  corpus = []\n",
    "  for filename  in directorio:\n",
    "     texto = open(path+filename,'r',encoding=\"utf-8\").read()\n",
    "     corpus.append(texto)\n",
    "  return(corpus,directorio)\n",
    "\n",
    "def PreProcesar(textos):\n",
    "    texto_limpio = []\n",
    "    for texto in textos:  \n",
    "        texto = EliminaNumeroYPuntuacion(texto)\n",
    "        texto = EliminarSaltosLinea(texto)\n",
    "        texto = EliminarEspaciosExtras(texto)\n",
    "        #texto = Lematizar(texto)\n",
    "        texto_limpio.append(texto)\n",
    "    return(texto_limpio)\n",
    "\n",
    "# Es necesario borrar símbolos que no estaban previamente identificados en puctuation\n",
    "def EliminaNumeroYPuntuacion(oracion):\n",
    "    string_numeros = regex.sub(r'[-\\.\\”\\“\\¿\\?\\!\\¡\\°\\d+\\…\\–\\´]','', oracion) \n",
    "    return(''.join(c for c in string_numeros if c not in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~¿¡°\\”\\“\\…\\–\\´')) #punctuation\n",
    "           \n",
    "def EliminarSaltosLinea(oracion):\n",
    "    string = regex.sub(r'[\\n\\r\\t\\xa0\\xad]',' ', oracion)\n",
    "    return(''.join(c for c in string))\n",
    "\n",
    "def EliminarEspaciosExtras(oracion):\n",
    "    string = regex.sub(r'\\s{2,}',' ', oracion)\n",
    "    return(''.join(c for c in string))\n",
    "\n",
    "\n",
    "def Similitud(Vec1,Vec2):\n",
    "    sim = 1 - cosine(Vec1, Vec2)\n",
    "    return(sim)\n",
    "\n",
    "def Lematizar(oracion):\n",
    "   doc = nlp(oracion)\n",
    "   lemas = [token.lemma_ for token in doc]\n",
    "   return(\" \".join(lemas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear el LSA y El Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"Discursos\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp    = es_core_news_sm.load()\n",
    "NumDim =  12\n",
    "corpus,directorio = CrearCorpus(PATH)\n",
    "\n",
    "CrearEspacioLSA(corpus,NumDim,\"mi_lsa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el modelo LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Sigma, vect_terms, vect_docs, lista_palabras) = CargarModeloLSA(\"mi_lsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = CrearDiccionario(vect_terms, lista_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Número de líneas del documento a generar\n",
    "N=20\n",
    "#selección aleatoria del primer documento.\n",
    "primer_documento=random.randint(0, len(corpus))\n",
    "\n",
    "# Añade la primera oración\n",
    "Texto=[sent_tokenize(corpus[primer_documento], language='spanish')[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(0,N):\n",
    "    #inicialización y penultima oración.\n",
    "    \n",
    "    # Esta parte es para considerar el número de frases que tiene el documento, \n",
    "    # para su aplicación en el cálculo del promedio\n",
    "    if i==0:\n",
    "        n_oracion=len(sent_tokenize(corpus[primer_documento], language='spanish'))\n",
    "    else:\n",
    "        n_oracion=len(Nuevo_Documento)\n",
    "        \n",
    "    #Seleccionamos la primera línea o la última añadida para compararla con la nueva a ingresar.\n",
    "    # se considera el promedio como la suma del vector de palabras de la oración, \n",
    "    # dividido por el número de oraciones detectadas en el documento.\n",
    "    \n",
    "    arregloO=word_tokenize(PreProcesar(Texto)[i].lower(), language='spanish')\n",
    "    #print(PreProcesar(Texto)[i].lower())\n",
    "    vector_oracionO=sum([terms[i] for i in arregloO if len(i)>1])/n_oracion \n",
    "    \n",
    "    # Elige el nuevo documento, cuya oración se podría añadir por similitud\n",
    "    nd=random.randint(0, len(corpus))\n",
    "    Nuevo_Documento=PreProcesar(sent_tokenize(corpus[nd], language='spanish'))\n",
    "\n",
    "\n",
    "    # Se recorre línea a línea del documento y se separan en sus palabras\n",
    "    valor=0\n",
    "    for j in range(0,len(Nuevo_Documento)):\n",
    "        linea=word_tokenize(Nuevo_Documento[j].lower(), language='spanish')\n",
    "        vector_oracion=sum([terms[i] for i in linea if len(i)>1])/len(Nuevo_Documento) #calcula el promedio\n",
    "        \n",
    "        similaridad=Similitud(vector_oracionO,vector_oracion)\n",
    "        \n",
    "        if (valor <= similaridad) & (similaridad!=1.0) : \n",
    "            #se descarta la similaridad ==1, pues en ese caso sería la misma oración\n",
    "            # nos quedamos con la mayor similitud\n",
    "            print(j)\n",
    "            print(similaridad)\n",
    "            print(Nuevo_Documento[j])\n",
    "            valor=similaridad\n",
    "            nueva_oracion=sent_tokenize(corpus[nd], language='spanish')[j] # se añade la oración de mayor similitud, pero se toma la original sin preprocesamiento.\n",
    "    \n",
    "    Texto.append(nueva_oracion)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se guarda el discurso generado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('texto_final.txt', \"w\")\n",
    "file.write(' '.join(regex.sub(r'[\\n\\r\\t\\xa0\\xad]',' ', linea) for linea in Texto))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
